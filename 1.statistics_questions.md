

## Table of contents


## Part 1: Inferential Statistics

1. What descriptive measure in statistics do you know?

    __Answer:__

  - Mean,median: an average or most commonly indicated response.
  - Standard deviation, variance: How "spread out" the  data are, which one is too large or too small
  - Percentile, Quartile:

    Percentile: You are (1.85m) the fourth tallest person in a group of 20, it mean 80% people are shorter than you, you are at 80th percentile or 1.85m is the 80th percentile.

    Quartile: split data into 4 group, Q1 (lower quartile - 25th percentile), Q2 (middle quartile or median - 50th percentile), Q3 (upper quartile - 75th percentile)

2. What’s the difference between the mean, the median and the mode?

    __Answer:__

|   Mean   |      Median     |      Mode     |
|:--------:|:---------------:|:-------------:|
|Mean is the average number (sum all sample / number of samples)|Median is the middle number, number of position (N/2)th |Mode is the most frequent number in sample|

3. What is the difference between Type I and Type II error?

    __Answer:__

    We usually refer this error in statistical hypothesis testing.

|    Type 1 error    |   Type 2 error     |
|:------------------:|:------------------:|
|False Positive (You say 1 but it is 0 - You reject a true null hypothesis)|False Negative (You say 0 but it is 1 - you accept a false null hypothesis)|

4. What is a p-value, and what is it used for?

    __Answer:__
     (draft) small p-value means we can reject the null hypothesis and accept the alternative hypothesis

5. What are the null and alternative hypothesis in a statistical test?
    __Answer:__

    Null and alternative hypothesis are two mutually exclusive statement. A hypothesis test uses sample data to determine whether to reject the null hypothesis.

  - Null hypothesis: A statement that a population parameter (such as mean, std, proportion) is equal to a hypothesized value.
  - Alternative hypothesis: A statement that a population parameter (such as mean, std, proportion) is less / greater/ different than a hypothesized value. The alternative hypothesis is what we might believe to be true or hope to prove true.


6. What is a t-test? Do you know what is its relationship to the z-test?

    __Answer:__
    t-test and z-test both base on an assumption that the null distribution is Normal Distribution
    |||
    |:---:|:---:|
    |Z-test|Z-test is used when the null distribution of test statistic is Standard Normal -> std = 1|
    |T-test|T-test is used when we don't know std of of the distribution|


7. What is the power of a statistical test?

    __Answer:__

    Probability of rejecting a false null hypothesis is __power of the test__ or Probability to avoid False Negative Error (Type II)

8. What is the standard deviation?

    __Answer__ Standard deviation tells you the variance of the data and which data point can be considered super large or super small (or a.k.a outliers)

9. What is a confidence interval, and what is it used for?

    __Answer__ Confidence interval is an interval that contains estimated parameter with a certain probability. For example 90% Confidence Interval means there is 90% chance that the interval will cover the true parameter.


10. What is bootstrapping/resampling used for?

    __Answer:__ Boostrap is used to estimate parameter by Monte Carlo simulations when it is too difficult to do it analytically. Boostrap bascially resample (choose a subset randomly) original data and estimate the parameter on those resample data.

11. Do you know the difference between frequentist and Bayesian statistics?

    __Answer:__


12. What are outliers? How can you check for them?

    __Answer:__

13. What is a correlation coefficient? What range of values can it take?

    __Answer:__

14. What is the Central Limit Theorem?

    __Answer:__

15. What is the normal distribution? How can you test if data is normally distributed?

    __Answer:__

16. Can you explain the major types of plots - histogram, bar chart, box plot, and scatter plot?

    __Answer:__

    - histogram: present distribution of the data, simply tell you how data are distributed in different ranges.
    - bar chart: represent value in a rectangle width x height, which height is corresponding to the value.
    - Box plot: represent data in a box with two tails Box plot tells you mean value, outliers, Q1, Q2, Q3.
    - scatter plot: show how data points locate in a 2D dimension.

17. What is a correlation matrix?

    __Answer:__ correlation matrix is a matrix of pair-wise correlation between features / predictors

## Part 2: Machine Learning

- Fundamentals
    - Can you make the distinction between an algorithm and a model?

      __Answer:__ Model is a function representing a data set, algorithms are a way to obtain that function

    - What’s the difference between supervised and unsupervised learning? Labeled vs. unlabeled data

      __Answer:__ supervised learning is based on labeled data while in unsupervised learning, data are unlabeled.

    - What’s the difference between a regression and a classification problem? How about clustering?

      __Answer:__ Regression is a supervised problem where the label is a continuous value like stock price, house price. classification is also a supervised problem but the label is a fixed length set like A,B,C,D. Clustering is a unsupervised problem where we want to find some groups of cluster of data.

    - Why do we use a train/test split?

      __Answer:__ Avoid overfitting and better generalization

    - What is cross-validation used for? What types of cross-validation do you know?

      __Answer:__ If we just fit the data on the training set, we would never use test set at training. It could be a waste of data, consider that data are very valuable. Cross-validation solves this problem by using all data but still keep generalization.

      Type of cross-validation:
      - leave-one-out CV
      - k-fold CV


    - What is generalization error?

      __Answer:__ The error on test set

    - What is the bias-variance trade off?

      __Answer:__ In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.

    - What is the difference between overfit and underfit?

      __Answer:__

      Overfit is when we use too complex function to capture a simple relation. We could see very high accuracy in training set but very low accuracy in test set.

      Underfit is when we use too simple function to capture a very complex relation. We sould see a low accuracy in both training set and test set.

    - Video: What accuracy metrics do you know, both for classification and for regression? When would you use one metric vs the other?

      __Answer:__


    - What is the curse of dimensionality?

      __Answer:__

    - Why do you need to set the random seed prior to running certain ML algorithms?

      __Answer:__


- Regression
    - Can you explain the difference between Linear and Logistic Regression?

      __Answer__ Linear Regression is for regression problem while logistic regression deals with classification problem.

    - How are the coefficients in a Linear Regression interpreted?

      __Answer__ Coefficient in linear regression can be interpreted as growth rate. For example, in house price
       prediction, coefficient of feature "Area" is 50, it means if "Area" of the house increase by 1, the house price increases by 50.

    - How is the intercept in a Linear Regression interpreted?

      __Answer__ The base value of response (Y) when predictor (X) = 0

    - Can the coefficients in a Logistic Regression be directly interpreted?

      __Answer__ No

    - What is the Adjusted R-Squared? What range of values can it take?

      __Answer__ Adjusted R-Squared is R-squared with penalty in number of features (add more useless features would increase R-squared but decrease Adjusted R-squared), it ranges from 0 to 1

    - Why is the Adjusted R-Squared a better measure than the regular R-Squared?

      __Answer__ Add more useless features would increase R-squared but decrease Adjusted R-squared.

    - How does Logistic Regression work “under the hood”? Can you explain Gradient Descent?

      __Answer__


- SVM
    - Can you explain how a Support Vector Machine works?

      __Answer__ SVM is a soft margin classifier. It means SVM will find a separate line that maximize margin between two side. SVM allows some violations, determine by parameter C, larger C -> more violations and vice versa.

    - What is the kernel trick?

      Adopted https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78

      __Answer__ In order to maximize the margin, we don't need exact data points X, we only neet its inner product X_transpose * X. So if we transform X into higher dimension (in many cases it can help us classify better), we only need compute its inner products only without actually visiting it. Cool ha?


    - Where does the “support vector” term come in the SVM name?

      __Answer__

    - What kind of kernels exist for SVMs?

      __Answer__  linear, polynomial, radical / Gaussian

- Trees
    - How do Decision Trees work?

      __Answer__

    - What criteria does a tree-based algorithm use to decide on a split?

      __Answer__ Gini Index or Entropy

    - How does the Random Forest algorithm work? What are the sources of randomness?

      __Answer__ Random Forest grows a tree on a subset of features randomly and average them to get the final result. The randomness in RF comes from choose random subset of feature to grow a decision tree. Usually, we pick number of subset = sqrt( number of features ). By this way, Random Forest decorrelates Decision Tree, therefore increase prediction power by reducing the variance.

    - How is feature importance calculated by the Random Forest?

      __Answer__ The feature importance in Random Forest is calculated by the mean (in many trees) decrease of Gini Index for each variable.

- Other Supervised Learning
    - What is the difference between Bagging and Boosting?

      __Answer__

      - Bagging: ensembling to create more data, training model on subset of data and combine by averaging / major vote them  
      - Boosting: Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.

    - How do Gradient Boosted Machines work?

      Adopted from: https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
      http://explained.ai/gradient-boosting/index.html

      http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/

      __Answer__
        - Create a simple model $F_{1}(x)=y$
        - Fit a model to the residuals $h_{1}(x)=y - F_{1}(x)$
        - Create a new model $F_{2}(x)=F_{1}(x) + h_{1}(x)$

    - What is Regularization, and what types do you know? Avoid overfitting, L1 and L2 regularization. L1 used as dim reduction, L2 better for overall generalization, bonus points for ElasticNet

      __Answer__

    - Can a Random Forest and a GBM be easily parallelized? Why/why not?

      __Answer__

- Unsupervised Learning
    - How does PCA work? What are the uses cases for it?

      __Answer__ PCA allow us to summarize the large data set with a smaller number of representative variables that collectively explain most of the variability in the original set. PCA is used to map the high-dimension data into 2D dimension for visualization.

    - How can you determine the optimal number of principal components?

      __Answer__

    - How does the K-Means algorithm work? What are its limitations?
    - What other clustering algorithms do you know?
    - How can you assess the quality of clustering?
    - Can you explain in detail any other clustering algorithms besides K-Means?

__Other__

1. Explain what regularization is and why it is useful.
2. Which data scientists do you admire most? which startups?
3. How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.
4. Explain what precision and recall are. How do they relate to the ROC curve?
5. How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything?
6. What is root cause analysis?
7. Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples.
8. What is statistical power?
9. Explain what resampling methods are and why they are useful. Also explain their limitations.
10. Is it better to have too many false positives, or too many false negatives? Explain.
11. What is selection bias, why is it important and how can you avoid it?
12. Give an example of how you would use experimental design to answer a question about user behavior.
13. What is the difference between "long" and "wide" format data?
14. What method do you use to determine whether the statistics published in an article (e.g. newspaper) are either wrong or presented to support the author's point of view, rather than correct, comprehensive factual information on a specific subject?
15. Explain Edward Tufte's concept of "chart junk."
16. How would you screen for outliers and what should you do if you find one?
17. How would you use either the extreme value theory, Monte Carlo simulations or mathematical statistics (or anything else) to correctly estimate the chance of a very rare event?
18. What is a recommendation engine? How does it work?
19. Explain what a false positive and a false negative are. Why is it important to differentiate these from each other?
20. Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?
https://www.edureka.co/blog/interview-questions/data-science-interview-questions/

## 99 Questions

Source: https://www.datasciencecentral.com/profiles/blogs/66-job-interview-questions-for-data-scientists

- What is the biggest data set that you processed, and how did you process it, what were the results?
- Tell me two success stories about your analytic or computer science projects? How was lift (or success) measured?
- What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?
- What is: collaborative filtering, n-grams, map reduce, cosine distance?
- How to optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?
- How would you come up with a solution to identify plagiarism?
- How to detect individual paid accounts shared by multiple users?
- Should click data be handled in real time? Why? In which contexts?
- What is better: good data or good models? And how do you define "good"? Is there a universal good model? Are there any models that are definitely not so good?
- What is probabilistic merging (AKA fuzzy merging)? Is it easier to handle with SQL or other languages? Which languages would you choose for semi-structured text data reconciliation?
- How do you handle missing data? What imputation techniques do you recommend?
- What is your favorite programming language / vendor? why?
- Tell me 3 things positive and 3 things negative about your favorite statistical software.
- Compare SAS, R, Python, Perl
- What is the curse of big data?
- Have you been involved in database design and data modeling?
- Have you been involved in dashboard creation and metric selection? What do you think about Birt?
- What features of Teradata do you like?
- You are about to send one million email (marketing campaign). How do you optimze delivery? How do you optimize response? - Can you optimize both separately? (answer: not really)
- Toad or Brio or any other similar clients are quite inefficient to query Oracle databases. Why? How would you do to increase speed by a factor 10, and be able to handle far bigger outputs?
- How would you turn unstructured data into structured data? Is it really necessary? Is it OK to store data as flat text files rather than in an SQL-powered RDBMS?
- What are hash table collisions? How is it avoided? How frequently does it happen?
- How to make sure a mapreduce application has good load balance? What is load balance?
- Examples where mapreduce does not work? Examples where it works very well? What are the security issues involved with the cloud? What do you think of EMC's solution offering an hybrid approach - both internal and external cloud - to mitigate the risks and offer other advantages (which ones)?
- Is it better to have 100 small hash tables or one big hash table, in memory, in terms of access speed (assuming both fit within RAM)? What do you think about in-database analytics?
- Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?
- Have you been working with white lists? Positive rules? (In the context of fraud or spam detection)
- What is star schema? Lookup tables?
- Can you perform logistic regression with Excel? (yes) How? (use linest on log-transformed data)? Would the result be good? (Excel has numerical issues, but it's very interactive)
- Have you optimized code or algorithms for speed: in SQL, Perl, C++, Python etc. How, and by how much?
- Is it better to spend 5 days developing a 90% accurate solution, or 10 days for 100% accuracy? Depends on the context?
- Define: quality assurance, six sigma, design of experiments. Give examples of good and bad designs of experiments.
- What are the drawbacks of general linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)?
- Do you think 50 small decision trees are better than a large one? Why?
- Is actuarial science not a branch of statistics (survival analysis)? If not, how so?
- Give examples of data that does not have a Gaussian distribution, nor log-normal. Give examples of data that has a very chaotic distribution?
- Why is mean square error a bad measure of model performance? What would you suggest instead?
- How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything? - Are you familiar with A/B testing?
- What is sensitivity analysis? Is it better to have low sensitivity (that is, great robustness) and low predictive power, or the other way around? How to perform good cross-validation? What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?
- Compare logistic regression w. decision trees, neural networks. How have these technologies been vastly improved over the last 15 years?
- Do you know / used data reduction techniques other than PCA? What do you think of step-wise regression? What kind of step-wise techniques are you familiar with? When is full data better than reduced data or sample?
- How would you build non parametric confidence intervals, e.g. for scores? (see the AnalyticBridge theorem)
- Are you familiar either with extreme value theory, monte carlo simulations or mathematical statistics (or anything else) to correctly estimate the chance of a very rare event?
- What is root cause analysis? How to identify a cause vs. a correlation? Give examples.
- How would you define and measure the predictive power of a metric?
- How to detect the best rule set for a fraud detection scoring technology? How do you deal with rule redundancy, rule discovery, and the combinatorial nature of the problem (for finding optimum rule set - the one with best predictive power)? - Can an approximate solution to the rule set problem be OK? How would you find an OK approximate solution? How would you decide it is good enough and stop looking for a better one?
- How to create a keyword taxonomy?
- What is a Botnet? How can it be detected?
- Any experience with using API's? Programming API's? Google or Amazon API's? AaaS (Analytics as a service)?
- When is it better to write your own code than using a data science software package?
- Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?
- What is POC (proof of concept)?
- What types of clients have you been working with: internal, external, sales / finance / marketing / IT people? Consulting experience? Dealing with vendors, including vendor selection and testing?
- Are you familiar with software life cycle? With IT project life cycle - from gathering requests to maintenance?
- What is a cron job?
- Are you a lone coder? A production guy (developer)? Or a designer (architect)?
- Is it better to have too many false positives, or too many false negatives?
- Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples.
- How does Zillow's algorithm work? (to estimate the value of any home in US)
- How to detect bogus reviews, or bogus Facebook accounts used for bad purposes?
- How would you create a new anonymous digital currency?
- Have you ever thought about creating a startup? Around which idea / concept?
- Do you think that typed login / password will disappear? How could they be replaced?
- Have you used time series models? Cross-correlations with time lags? Correlograms? Spectral analysis? Signal processing and filtering techniques? In which context?
- Which data scientists do you admire most? which startups?
- How did you become interested in data science?
- What is an efficiency curve? What are its drawbacks, and how can they be overcome?
- What is a recommendation engine? How does it work?
- What is an exact test? How and when can simulations help us when we do not use an exact test?
- What do you think makes a good data scientist?
- Do you think data science is an art or a science?
- What is the computational complexity of a good, fast clustering algorithm? What is a good clustering algorithm? How do you determine the number of clusters? How would you perform clustering on one million unique keywords, assuming you have 10 million data points - each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?
- Give a few examples of "best practices" in data science.
- What could make a chart misleading, difficult to read or interpret? What features should a useful chart have?
- Do you know a few "rules of thumb" used in statistical or computer science? Or in business analytics?
- What are your top 5 predictions for the next 20 years?
- How do you immediately know when statistics published in an article (e.g. newspaper) are either wrong or presented to support the author's point of view, rather than correct, comprehensive factual information on a specific subject? For instance, what do you think about the official monthly unemployment statistics regularly discussed in the press? What could make them more accurate?
- Testing your analytic intuition: look at these three charts. Two of them exhibit patterns. Which ones? Do you know that these charts are called scatter-plots? Are there other ways to visually represent this type of data?
- You design a robust non-parametric statistic (metric) to replace correlation or R square, that (1) is independent of sample size, (2) always between -1 and +1, and (3) based on rank statistics. How do you normalize for sample size? Write an algorithm that computes all permutations of n elements. How do you sample permutations (that is, generate tons of random permutations) when n is large, to estimate the asymptotic distribution for your newly created metric? You may use this asymptotic distribution for normalizing your metric. Do you think that an exact theoretical distribution might exist, and therefore, we should find it, and use it rather than wasting our time trying to estimate the asymptotic distribution using simulations?
- More difficult, technical question related to previous one. There is an obvious one-to-one correspondence between permutations of n elements and integers between 1 and n! Design an algorithm that encodes an integer less than n! as a permutation of n elements. What would be the reverse algorithm, used to decode a permutation and transform it back into a number? Hint: An intermediate step is to use the factorial number system representation of an integer. Feel free to check this reference online to answer the question. Even better, feel free to browse the web to find the full answer to the question (this will test the candidate's ability to quickly search online and find a solution to a problem without spending hours reinventing the wheel).  
- How many "useful" votes will a Yelp review receive? My answer: Eliminate bogus accounts (read this article), or competitor reviews (how to detect them: use taxonomy to classify users, and location - two Italian restaurants in same Zip code could badmouth each other and write great comments for themselves). Detect fake likes: some companies (e.g. FanMeNow.com) will charge you to produce fake accounts and fake likes. Eliminate prolific users who like everything, those who hate everything. Have a blacklist of keywords to filter fake reviews. See if IP address or IP block of reviewer is in a blacklist such as "Stop Forum Spam". Create honeypot to catch fraudsters.  Also watch out for disgruntled employees badmouthing their former employer. Watch out for 2 or 3 similar comments posted the same day by 3 users regarding a company that receives very few reviews. Is it a brand new company? Add more weight to trusted users (create a category of trusted users).  Flag all reviews that are identical (or nearly identical) and come from same IP address or same user. Create a metric to measure distance between two pieces of text (reviews). Create a review or reviewer taxonomy. Use hidden decision trees to rate or score review and reviewers.
- What did you do today? Or what did you do this week / last week?
- What/when is the latest data mining book / article you read? What/when is the latest data mining conference / webinar / class / workshop / training you attended? What/when is the most recent programming skill that you acquired?
- What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?
- What/when/where is the last data science blog post you wrote?
- In your opinion, what is data science? Machine learning? Data mining?
- Who are the best people you recruited and where are they today?
- Can you estimate and forecast sales for any book, based on Amazon public data? Hint: read this article.
- What's wrong with this picture?
- Should removing stop words be Step 1 rather than Step 3, in the search engine algorithm described here? Answer: Have you thought about the fact that mine and yours could also be stop words? So in a bad implementation, data mining would become data mine after stemming, then data. In practice, you remove stop words before stemming. So Step 3 should indeed become step 1.
- Experimental design and a bit of computer science with Lego's
